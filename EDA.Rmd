---
title: "EDA"
author: "Team_1"
date: "5/29/2020"
output: html_document 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Package Mang, echo=FALSE}
# Install/Import Packages
install.packages("naniar")
install.packages("corrplot")
install.packages("funModeling")
install.packages("tidyverse")
install.packages("Hmisc")
install.packages("mice")
install.packages("glmnet")
install.packages("mltools")


library(naniar)
library(ggplot2)
library(corrplot)
library(dplyr)
library(caret)
library(funModeling)
library(tidyverse) 
library(Hmisc)
library(mice)
library(randomForest)
library(glmnet)
library(data.table)
library(Matrix)
library(caret)
library(mltools)

```

```{r Import, echo=FALSE}
# Import Data Sets
moddata = read.csv("modelingData.csv", header = TRUE)
projdata = read.csv("projectionData.csv", header = TRUE)

# Identify Structure and Stats
summary(moddata)
str(moddata)
summary(projdata)
str(projdata)
```

Data Cleaning and Wrangling

```{r Data Wrangling, echo=FALSE}
# Data Cleaning / Wrangling (any renaming of variables or standardizing of values.)
moddata_imp = mice(moddata, m=5, maxit=5, method='cart', seed = 500)
moddata_imp2 <- complete(moddata_imp, "long", inc = TRUE)

projdata_imp = mice(moddata, m=5, maxit=5, method='cart', seed = 500)
projdata_imp2 = complete(projdata_imp, "long", inc = TRUE)

train_df = moddata_imp2 %>% filter(.imp == 5)
test_df = projdata_imp2 %>% filter(.imp == 5)

train_df <- na.omit(train_df)
test_df <- na.omit(test_df)

```

Exploratory Data Analysis

```{r Outlier, echo=FALSE}
# Outlier Identification and Handling

# Function to Cap the Outliers using IQR
outlierTreament<-function(x){
  qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
  caps <- quantile(x, probs=c(.05, .95), na.rm = T)
  H <- 1.5 * IQR(x, na.rm = T)
  x[x < (qnt[1] - H)] <- caps[1]
  x[x > (qnt[2] + H)] <- caps[2]
  return(x)}

# Remove String Columns 
numeric_cols<-test_df[sapply(test_df, is.numeric)]
numeric_data<-test_df[,test_df%in%numeric_cols]

numeric_cols2<-train_df[sapply(train_df, is.numeric)]
numeric_data2<-train_df[,train_df%in%numeric_cols]

# Apply IQR Functions to DBs
test_df_IQR<-as.data.frame(sapply(numeric_data,outlierTreament))
train_df_IQR<-as.data.frame(sapply(numeric_data2,outlierTreament))

summary(test_df_IQR)
summary(train_df_IQR)
```

```{r Miss Value, echo=FALSE}
# Missing value identification, summary and possible imputation (mean, median, regression.) This may also be considered part of “Data Wrangling”.

#Create plots to analyize missing data
gg_miss_var(moddata)
vis_miss(moddata, warn_large_data = FALSE)
gg_miss_var(projdata)
vis_miss(projdata, warn_large_data = FALSE)
```

```{r}

############################################
#Address missing values in each column
############################################
#missing values
# create new dataset without missing data
moddata <- na.omit(moddata)


```

```{r}

############################################
#basic plots to analyize data
############################################


# Plot to show the count by Product_type (Investment vs OwnerOccupie)
prodCount <- moddata %>%
  group_by(product_type) %>%
  summarise(counts = n())

ggplot(prodCount, aes(x = product_type, y = counts)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) + 
  theme_grey()

#PLot Price_doc vs full_sq
ggplot(aes(x=full_sq, y=price_doc), data=moddata) + 
    geom_point(color='blue')


# % of missed data by features
miss_pct <- map_dbl(moddata, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })
miss_pct <- miss_pct[miss_pct > 0]
data.frame(miss=miss_pct, var=names(miss_pct), row.names=NULL) %>%
    ggplot(aes(x=reorder(var, -miss), y=miss)) + 
    geom_bar(stat='identity', fill='blue') +
    labs(x='', y='% missing', title='Percent missing data by feature') +
    theme(axis.text.x=element_text(angle=90, hjust=1))

# Histogram showing build year distribution
moddata %>% 
    filter(build_year > 1940 & build_year < 2018) %>%
    ggplot(aes(x=build_year)) + 
    geom_histogram(fill='blue') + 
    ggtitle('Distribution of build year')

table(moddata$build_year)

# Date features
ts1 = as.Date(timestamp)
data[,":="(date_yday=yday(timestamp)
           ,date_month=month(timestamp)
           ,date_year=year(timestamp)
           ,date_week=week(timestamp)
           ,date_mday=mday(timestamp)
           ,date_wday=wday(timestamp)
)]


str(moddata) 

```

```{r Multicollinearity, echo=FALSE}
# Multicollinearity (is there reason to believe it is present?)  You don’t have to address every potential pair of variables that may be collinear.  Just provide a plot and or other evidence of a single occurrence of multicollinearity if at least one exists and then mention possible other occurrences.  

#Return numeric values only
df_numeric <- moddata[, sapply(moddata, is.numeric)]

#Correlation Plot
df_numeric[is.na(df_numeric)] <- "0"
df_numeric <- df_numeric[, sapply(df_numeric, is.numeric)]
df_corr <- round(cor(df_numeric),2)

corrplot(cor(df_corr), diag = FALSE, order = "FPC",
         tl.pos = "td", tl.cex = 0.5, method = "color", type = "upper")

```

```{r Assumptions, echo=FALSE}
# Homoscedasticity, normal distributions of the response for fixed values of the explanatory variable(s), linear relationship between the mean of the response and each explanatory variable, etc.  This is where you would apply transformations (log, square root, etc.)

histogram(moddata$price_doc)
moddata["log_price_doc"] = log(moddata$price_doc,10)
histogram(moddata$log_price_doc)

##Function to remove outliers
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

#Testing outliers
boxplot(moddata$num_room)
boxplot(moddata$max_floor)
boxplot(moddata$full_sq)
boxplot(moddata$full_sq)

moddata$max_floor <- remove_outliers(moddata$max_floor)
moddata$full_sq <- remove_outliers(moddata$full_sq)


```

```{r Variable Selection, echo=FALSE}
# Variable selection: For example, there are many potential explanatory variables. Running stepwise variable selection will not necessarily provide a final model, but may leave you with a smaller set of potential explanatory variables to work with.

```

```{r Miscellaneous, echo=FALSE}
# Anything else that might be appropriate in learning about the data before getting started.  (Example: You might analyze interactions between explanatory variables in the analysis.)

```

```{r Model building, echo=FALSE}
# Model using Stepwise Elimination Process)


#Feature Engineering
moddata["building_age"] = 2020 - moddata$build_year
moddata["diff_floor"] = moddata$max_floor-moddata$floor

#One Hot Encoding for Invetment Categorical Variable
newdata <- one_hot(as.data.table(moddata$product_type))
moddata["V1_Investment"] = newdata$V1_Investment
moddata["V1_OwnerOccupier"] = newdata$V1_OwnerOccupier

#Data Partision
set.seed(100) 
index = sample(1:nrow(moddata), 0.7*nrow(moddata)) 

train = moddata[index,] # Create the training data 
test = moddata[-index,] # Create the test data

dim(train)
dim(test)

#OLS Model with StepWise
OLS_model = step(
            lm(log_price_doc~
                  timestamp+
                  V1_Investment+V1_OwnerOccupier+
            
                  full_sq+floor+hospital_beds_raion+life_sq+num_room+max_floor+raion_popul+
                  kitch_sq+preschool_quota+
                  healthcare_centers_raion+university_top_20_raion+shopping_centers_raion+
                  metro_km_walk+
                  healthcare_centers_raion+university_top_20_raion+shopping_centers_raion+
                 
                  build_count_block+build_count_wood+build_count_frame+build_count_brick+
                 
                  build_count_before_1920+
                 
                  kremlin_km+big_road1_km+big_road2_km+railroad_km+bus_terminal_avto_km+big_market_km+market_shop_km+
                  
                  office_raion+railroad_terminal_raion+
                  railroad_station_avto_min+public_transport_station_km+ice_rink_km+
                  swim_pool_km+fitness_km+university_km+
               
                  X7_14_all+X0_17_all+X16_29_all+X0_13_all
               
               
                , data = train), direction="forward")
summary(OLS_model)

summary(moddata$price_doc)

#Function to evaluate metrics
eval_metrics = function(model, df, predictions, target){
    resids = df[,target] - predictions
    resids2 = resids**2
    N = length(predictions)
    r2 = as.character(round(summary(model)$r.squared, 2))
    adj_r2 = as.character(round(summary(model)$adj.r.squared, 2))
    print(adj_r2) #Adjusted R-squared
    print(as.character(round(sqrt(sum(resids2)/N), 2))) #RMSE
}


#predicting and evaluating the model on train data
predictions = predict(OLS_model, newdata = train)
eval_metrics(OLS_model, train, predictions, target = 'log_price_doc')

#predicting and evaluating the model on test data
predictions = predict(OLS_model, newdata = test)
eval_metrics(OLS_model, test, predictions, target = 'log_price_doc')

a = colnames(moddata)
a

str(moddata)

###**Removed Features ####
#school_km+park_km
#children_school
#material+full_all
#build_year
#full_all+X0_6_all
#+green_zone_part
#+stadium_km+basketball_km+big_church_km+workplaces_km
#big_market_raion
#build_count_1921.1945+build_count_1946.1970+build_count_1971.1995+build_count_after_1995+


##LASSO Regression (Second regresstion method)
lambdas <- 10^seq(2, -3, by = -.1)
lasso_reg = glmnet(x=df_numeric, y=df_numeric$price_doc, alpha = 1, lambda = lambdas, standardize = TRUE)

##Random Forest
rf_classifier = randomForest(log_price_doc~full_sq+children_school, 
                             data=df_numeric, ntree=10, mtry=1, importance=TRUE)
summary(rf_classifier)

# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best


```













