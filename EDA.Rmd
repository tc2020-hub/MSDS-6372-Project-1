---
title: "EDA"
author: "Team_1"
date: "5/29/2020"
output: html_document 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Package Mang, echo=FALSE}
# Install/Import Packages
install.packages("naniar")
install.packages("corrplot")
install.packages("funModeling")
install.packages("tidyverse")
install.packages("Hmisc")
install.packages("mice")

library(naniar)
library(ggplot2)
library(corrplot)
library(dplyr)
library(caret)
library(funModeling)
library(tidyverse) 
library(Hmisc)
library(mice)

```

```{r Import, echo=FALSE}
# Import Data Sets
moddata = read.csv("modelingData.csv", header = TRUE)
projdata = read.csv("projectionData.csv", header = TRUE)

# Identify Structure and Stats
summary(moddata)
str(moddata)
summary(projdata)
str(projdata)
```

Data Cleaning and Wrangling

```{r Data Wrangling, echo=FALSE}
# Data Cleaning / Wrangling (any renaming of variables or standardizing of values.)
moddata_imp = mice(moddata, m=5, maxit=5, method='cart', seed = 500)
moddata_imp2 <- complete(moddata_imp, "long", inc = TRUE)

projdata_imp = mice(moddata, m=5, maxit=5, method='cart', seed = 500)
projdata_imp2 = complete(projdata_imp, "long", inc = TRUE)

train_df = moddata_imp2 %>% filter(.imp == 5)
test_df = projdata_imp2 %>% filter(.imp == 5)

train_df <- na.omit(train_df)
test_df <- na.omit(test_df)

```

Exploratory Data Analysis

```{r Outlier, echo=FALSE}
# Outlier Identification and Handling

# Function to Cap the Outliers using IQR
outlierTreament<-function(x){
  qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
  caps <- quantile(x, probs=c(.05, .95), na.rm = T)
  H <- 1.5 * IQR(x, na.rm = T)
  x[x < (qnt[1] - H)] <- caps[1]
  x[x > (qnt[2] + H)] <- caps[2]
  return(x)}

# Remove String Columns 
numeric_cols<-test_df[sapply(test_df, is.numeric)]
numeric_data<-test_df[,test_df%in%numeric_cols]

numeric_cols2<-train_df[sapply(train_df, is.numeric)]
numeric_data2<-train_df[,train_df%in%numeric_cols]

# Apply IQR Functions to DBs
test_df_IQR<-as.data.frame(sapply(numeric_data,outlierTreament))
train_df_IQR<-as.data.frame(sapply(numeric_data2,outlierTreament))

summary(test_df_IQR)
summary(train_df_IQR)
```

```{r Miss Value, echo=FALSE}
# Missing value identification, summary and possible imputation (mean, median, regression.) This may also be considered part of “Data Wrangling”.

#Create plots to analyize missing data
gg_miss_var(moddata)
vis_miss(moddata, warn_large_data = FALSE)
gg_miss_var(projdata)
vis_miss(projdata, warn_large_data = FALSE)
```

```{r}

############################################
#basic plots to analyize data
############################################


# Plot to show the count by Product_type (Investment vs OwnerOccupie)
prodCount <- moddata %>%
  group_by(product_type) %>%
  summarise(counts = n())

ggplot(prodCount, aes(x = product_type, y = counts)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) + 
  theme_grey()

#PLot Price_doc vs full_sq
ggplot(aes(x=full_sq, y=price_doc), data=moddata) + 
    geom_point(color='blue')

# price_doc is right skwed we can 
ggplot(moddata,aes(x=price_doc))+geom_density(fill="blue",alpha=0.6)
moddata['log_price_doc'] = log(moddata$price_doc)
ggplot(moddata,aes(x=log_price_doc))+geom_density(fill="blue",alpha=0.6)

# % of missed data by features
miss_pct <- map_dbl(moddata, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })
miss_pct <- miss_pct[miss_pct > 0]
data.frame(miss=miss_pct, var=names(miss_pct), row.names=NULL) %>%
    ggplot(aes(x=reorder(var, -miss), y=miss)) + 
    geom_bar(stat='identity', fill='blue') +
    labs(x='', y='% missing', title='Percent missing data by feature') +
    theme(axis.text.x=element_text(angle=90, hjust=1))

# Histogram showing build year distribution
moddata %>% 
    filter(build_year > 1940 & build_year < 2018) %>%
    ggplot(aes(x=build_year)) + 
    geom_histogram(fill='blue') + 
    ggtitle('Distribution of build year')

table(moddata$build_year)


str(moddata) 

```

```{r Multicollinearity, echo=FALSE}
# Multicollinearity (is there reason to believe it is present?)  You don’t have to address every potential pair of variables that may be collinear.  Just provide a plot and or other evidence of a single occurrence of multicollinearity if at least one exists and then mention possible other occurrences.  

#Return numeric values only
df_numeric <- moddata[, sapply(moddata, is.numeric)]

#Correlation Plot
df_numeric[is.na(df_numeric)] <- "0"
df_numeric <- df_numeric[, sapply(df_numeric, is.numeric)]
df_corr <- round(cor(df_numeric),2)

corrplot(cor(df_corr), diag = FALSE, order = "FPC",
         tl.pos = "td", tl.cex = 0.5, method = "color", type = "upper")

```

```{r Assumptions, echo=FALSE}
# Homoscedasticity, normal distributions of the response for fixed values of the explanatory variable(s), linear relationship between the mean of the response and each explanatory variable, etc.  This is where you would apply transformations (log, square root, etc.)

```

```{r Variable Selection, echo=FALSE}
# Variable selection: For example, there are many potential explanatory variables. Running stepwise variable selection will not necessarily provide a final model, but may leave you with a smaller set of potential explanatory variables to work with.

```

```{r Miscellaneous, echo=FALSE}
# Anything else that might be appropriate in learning about the data before getting started.  (Example: You might analyze interactions between explanatory variables in the analysis.)

```













